"Modelo de censura , regresion lineal con Modelo Tobit"
#tareas a realizar:
    
#estimar el modelo OLS (minimos cuadrados)
#estimar el modelo Tobit

###############################################################################
###############################################################################
###############################################################################
"Estimacion del Modelo Tobit"

import math
import warnings

import numpy as np
import pandas as pd
from scipy.optimize import minimize
import scipy.stats
from scipy.special import log_ndtr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error


def split_left_right_censored(x, y, cens):
    counts = cens.value_counts()
    if -1 not in counts and 1 not in counts:
        warnings.warn("No censored observations; use regression methods for uncensored data")
    xs = []
    ys = []

    for value in [-1, 0, 1]:
        if value in counts:
            split = cens == value
            y_split = np.squeeze(y[split].values)
            x_split = x[split].values

        else:
            y_split, x_split = None, None
        xs.append(x_split)
        ys.append(y_split)
    return xs, ys


def tobit_neg_log_likelihood(xs, ys, params):
    x_left, x_mid, x_right = xs
    y_left, y_mid, y_right = ys

    b = params[:-1]
    # s = math.exp(params[-1])
    s = params[-1]

    to_cat = []

    cens = False
    if y_left is not None:
        cens = True
        left = (y_left - np.dot(x_left, b))
        to_cat.append(left)
    if y_right is not None:
        cens = True
        right = (np.dot(x_right, b) - y_right)
        to_cat.append(right)
    if cens:
        concat_stats = np.concatenate(to_cat, axis=0) / s
        log_cum_norm = scipy.stats.norm.logcdf(concat_stats)  # log_ndtr(concat_stats)
        cens_sum = log_cum_norm.sum()
    else:
        cens_sum = 0

    if y_mid is not None:
        mid_stats = (y_mid - np.dot(x_mid, b)) / s
        mid = scipy.stats.norm.logpdf(mid_stats) - math.log(max(np.finfo('float').resolution, s))
        mid_sum = mid.sum()
    else:
        mid_sum = 0

    loglik = cens_sum + mid_sum

    return - loglik


def tobit_neg_log_likelihood_der(xs, ys, params):
    x_left, x_mid, x_right = xs
    y_left, y_mid, y_right = ys

    b = params[:-1]
    # s = math.exp(params[-1]) # in censReg, not using chain rule as below; they optimize in terms of log(s)
    s = params[-1]

    beta_jac = np.zeros(len(b))
    sigma_jac = 0

    if y_left is not None:
        left_stats = (y_left - np.dot(x_left, b)) / s
        l_pdf = scipy.stats.norm.logpdf(left_stats)
        l_cdf = log_ndtr(left_stats)
        left_frac = np.exp(l_pdf - l_cdf)
        beta_left = np.dot(left_frac, x_left / s)
        beta_jac -= beta_left

        left_sigma = np.dot(left_frac, left_stats)
        sigma_jac -= left_sigma

    if y_right is not None:
        right_stats = (np.dot(x_right, b) - y_right) / s
        r_pdf = scipy.stats.norm.logpdf(right_stats)
        r_cdf = log_ndtr(right_stats)
        right_frac = np.exp(r_pdf - r_cdf)
        beta_right = np.dot(right_frac, x_right / s)
        beta_jac += beta_right

        right_sigma = np.dot(right_frac, right_stats)
        sigma_jac -= right_sigma

    if y_mid is not None:
        mid_stats = (y_mid - np.dot(x_mid, b)) / s
        beta_mid = np.dot(mid_stats, x_mid / s)
        beta_jac += beta_mid

        mid_sigma = (np.square(mid_stats) - 1).sum()
        sigma_jac += mid_sigma

    combo_jac = np.append(beta_jac, sigma_jac / s)  # by chain rule, since the expression above is dloglik/dlogsigma

    return -combo_jac

#se define una clase llamada tobit para estimar un modelo de regresion (OLS) con tobit
class TobitModel:
    def __init__(self, fit_intercept=True):
        self.fit_intercept = fit_intercept
        self.ols_coef_ = None
        self.ols_intercept = None
        self.coef_ = None
        self.intercept_ = None
        self.sigma_ = None

    def fit(self, x, y, cens, verbose=False):
        """
        Fit a maximum-likelihood Tobit regression
        :param x: Pandas DataFrame (n_samples, n_features): Data
        :param y: Pandas Series (n_samples,): Target
        :param cens: Pandas Series (n_samples,): -1 indicates left-censored samples, 0 for uncensored, 1 for right-censored
        :param verbose: boolean, show info from minimization
        :return:
        """
        x_copy = x.copy()
        if self.fit_intercept:
            x_copy.insert(0, 'intercept', 1.0)
        else:
            x_copy.scale(with_mean=True, with_std=False, copy=False)
        init_reg = LinearRegression(fit_intercept=False).fit(x_copy, y)
        b0 = init_reg.coef_
        y_pred = init_reg.predict(x_copy)
        resid = y - y_pred
        resid_var = np.var(resid)
        s0 = np.sqrt(resid_var)
        params0 = np.append(b0, s0)
        xs, ys = split_left_right_censored(x_copy, y, cens)

        result = minimize(lambda params: tobit_neg_log_likelihood(xs, ys, params), params0, method='BFGS',
                          jac=lambda params: tobit_neg_log_likelihood_der(xs, ys, params), options={'disp': verbose})
        if verbose:
            print(result)
        self.ols_coef_ = b0[1:]
        self.ols_intercept = b0[0]
        if self.fit_intercept:
            self.intercept_ = result.x[1]
            self.coef_ = result.x[1:-1]
        else:
            self.coef_ = result.x[:-1]
            self.intercept_ = 0
        self.sigma_ = result.x[-1]
        return self

    def predict(self, x):
        return self.intercept_ + np.dot(x, self.coef_)

    def score(self, x, y, scoring_function=mean_absolute_error):
        y_pred = np.dot(x, self.coef_)
        return scoring_function(y, y_pred)

###############################################################################
###############################################################################
###############################################################################

import pandas as pd 
import statsmodels.api as sm
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.iolib.summary2 import summary_col
from tobit import *  #asterisco es para importar todas las funciones de esa libreria

ruta = "C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\Gas_LP.dta"  
Gaslp_df = pd.read_stata(ruta)
Gaslp_df.head() 

ruta2 = "C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\Gas_LP.csv"  
Gaslp_df2 = pd.read_csv(ruta2)
Gaslp_df2.head() 

###############################################################################

"Regresion simple"

Gaslp = Gaslp_df[Gaslp_df.gasto_gasnat == 0] #se elimina de la tabla de datos los hogares que consumen gas natural.
Gaslp

#para ver que valores de la matriz tienen NaN 
Gaslp.lgasto_gaslp
#se remplazan los NaN  con valores de 0
Gaslp['lgasto_gaslp'].fillna(0, inplace=True)
#para mostrar los valores y ver la correccion
Gaslp.lgasto_gaslp

#se busca usar un valor real donde se pueda identificar donde hay censura, 
#la variable de interes es el logaritmo natural del gasto de los hogares (lgasto_gaslp)
#se busca interpretar los coeficientes com elasticidades, estimando logaritmo del consumo y del precio
#por lo que el coeficiente asociado a la variable tendra funcion de elasticidad
#el cero es el valor b donde se observa la censura, valores de 0 es donde inicia la censura de los datos.

#se borran los valores NaN de la muestra:(de las otras variables que no son el log del gasto de gas lp)
"si se tienen otros NaN se puede rellenar los datos con algun metodo de correcion de missing values"

#pero en este caso se van a borrar los otros datos con NaN  / se reduce la potencia de la regresion porque la muestra se hizo mas peque√±a
Gaslp2  = Gaslp.dropna(how='any')
Gaslp2

#grafica de barras
plt.hist(Gaslp2.gasto_gaslp, normed=True, histtype='bar')

plt.ylabel('Gasto en Gas LP')
plt.title('Histogram')

plt.show()

Gaslp2['cens'] = 0 #se crea una columna cents con valor igual a 0
#con la funacion loc localiza del datafram la funcion lgasto_gaslp, identifica a los que tienen el valor 0
Gaslp2.loc[(Gaslp2.lgasto_gaslp == 0), 'cens'] = -1 #los que tengan valor 0, en la columna cens se remplaza por el valor -1
Gaslp2
 
#el algoritmo del modelo tobit requiere identificar los datos con censura y los datos sin censura 
#datos que tienen el valor de  lgasto_gaslp = 0 son censurados , los que no, no estan censurados

###############################################################################

#para definir variables
Y = Gaslp2.lgasto_gaslp
X = Gaslp2[['ling_cor', 'ling_cor2', 'tot_integ', 'lp_gaslp', 'acc_gasnat']] 
#acc_gasnat es una dummy que indica de ubicacion geografica sobre si hay probedor de gas natural 
Y

#variable de censura  / para contabilizar la censura del modelo 
cens = pd.Series(Gaslp2.cens) 
cens.value_counts()
#8713 variables con censura / 5304 variables sin censura 

"Estimacion del modelo Tobit con regresion"  #para hacer la estimacion del tobit (problema con la libreria)
tr = TobitModel()
tr = tr.fit(X, Y, cens, verbose = True)
#matrix X, vector Y, variable censurado, variables no censuradas, matriz g-siana
#hess_inv = para calcular las varianzas y desviaciones estandar de los coeficientes
#matriz hessiana_inversa hace la inversa de la matriz, despues se toman elementos de la diagonal, para encontrar la varianza
#de los coeficientes y construir la prueba z, 

tr.intercept_, tr.coef_ #el intercepto trae la constante del modelo considerando la censura
#Trae los ecoeficientes del modelo considerando la censura
#muestra constante del modelo tobit
#muestra los valores de los coeficientes, correspodiente a las variables por orden

###############################################################################
#(tiene rendimientos decrecientes la forma de las variables de ingreso con el consumo de gas)
#el consumo de gas lp es un bien normal para ingresos bajos
#a mayor es una variacion del precio o de una unidad disminuye siete valores el consumo

tr.sigma_

#ADECUACIONES 
x = sm.add_constant(X)
#Modelo OLS
model = sm.OLS(Y, X)
results = model.fit()
results.summary()

#resultados del tobit 
tr.coef_

#comparacion del tobit:
tr.intercept_, tr.coef_[0], tr.coef_[1], tr.coef_[2], tr.coef_[3], tr.coef_[4]
results.params




