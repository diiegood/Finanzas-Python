"""
####################### Nombre Modulo : funciones_curso #######################
"""


"""
Nota:
    
Al crear una funcion se debe cargar el modulo donde se encuentra y a su vez cargar la funcion a usar
cada vez que se crea una se debe reiniciar el programa para que la pueda correr sin problemas
correr primero el modulo de funciones y despues el principal.
"""

import pandas as pd
import numpy as np
from scipy.stats import norm
from scipy.optimize import minimize
import math

def drawdown(return_series: pd.Series):
    """Takes a time series of asset returns.
       returns a DataFrame with columns for
       the wealth index, 
       the previous peaks, and 
       the percentage drawdown
    """
    wealth_index = 1000*(1+return_series).cumprod()
    previous_peaks = wealth_index.cummax()
    drawdowns = (wealth_index - previous_peaks)/previous_peaks
    return pd.DataFrame({"Wealth": wealth_index, 
                         "Previous Peak": previous_peaks, 
                         "Drawdown": drawdowns})


def obtener_returnos():
    """
    Load the Fama-French Dataset for the returns of the Top and Bottom Deciles by MarketCap
    1- carga los datos panel
    2-filtra solo los valores de Lo-20 y Hi-20 del data frame
    3-se calculan los rendimientos de rendimientos/100
    4-se cambia el formato a serie de tiempo y se indexa el periodo
    5-regresa la funcion rets que es el dataframe ya filtrado con todo lo anterior
    """
    #se cargan los datos panel
    me_m = pd.read_csv(
       "C:/Users/creep/OneDrive/Escritorio/programacion/data/Portfolios_Formed_on_ME_monthly_EW.csv",
       header=0,
       index_col=0,
       parse_dates=True,
       na_values=-99.99)
    rets = me_m[['Lo 20', 'Hi 20']]
    #rets.columns = ['SmallCap', 'LargeCap']
    rets = me_m
    rets = rets/100
    rets.index = pd.to_datetime(rets.index, format="%Y%m").to_period('M')
    return rets

def obtener_retornos_totales():
    """
    Load the 30 industry portfolio data and derive the returns of a capweighted total market index
    """
    ind_nfirms = get_ind_nfirms()
    ind_size = get_ind_size()
    ind_return = rendimientos_industria()
    ind_mktcap = ind_nfirms * ind_size
    total_mktcap = ind_mktcap.sum(axis=1)
    ind_capweight = ind_mktcap.divide(total_mktcap, axis="rows")
    total_market_return = (ind_capweight * ind_return).sum(axis="columns")
    return total_market_return


def returns():  #funcion para leer base de datos
    """
    Load and format the EDHEC Hedge Fund Index Returns
    """
    hfi = pd.read_csv("C:/Users/creep/OneDrive/Escritorio/programacion/data/edhec-hedgefundindices.csv",
                      header=0, index_col=0, parse_dates=True)
    hfi = hfi/100
    hfi.index = hfi.index.to_period('M')
    return hfi


def skewness(r):
    """
    Alternative to scipy.stats.skew()
    Computes the skewness of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean()
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**3).mean()
    return exp/sigma_r**3


def kurtosis(r):
    """
    Alternative to scipy.stats.kurtosis()
    Computes the kurtosis of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean() 
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**4).mean()
    return exp/sigma_r**4


import scipy.stats
def is_normal(r, level=0.01):
    """
    Applies the Jarque-Bera test to determine if a Series is normal or not
    Test is applied at the 1% level by default
    Returns True if the hypothesis of normality is accepted, False otherwise
    """
    if isinstance(r, pd.DataFrame):
        return r.aggregate(is_normal)
    else:
        statistic, p_value = scipy.stats.jarque_bera(r)
        return p_value > level

def semi_desviacion(r):
    """
    los rendimientos con semi-desviacion o semidesvaicion negativa de r
    deben ser series o un data frame
    """
    is_negative = r<0
    return r[is_negative].std(ddof=0)

#r son los rendimientos
#level es el valor del percentil donde se va parar para sacar el VaR
def VaR_historico(r, level=5):
    """
    VaR historico
    """
    if isinstance(r, pd.DataFrame):
        return r.aggregate(VaR_historico, level=level)
    
    elif isinstance(r, pd.Series):
        return -np.percentile(r, level)
    else:
        raise TypeError("no seas wey")

      
    
def VaR_Normal (r, level=5):
    """
    Calcula el VaR con retornos usando modelo parametrico de la distribucion normal
    """
    #para calcular el valor z de la distribucion si fuera normal
    valor_dist_norm = norm.ppf(level/100)
    return -(r.mean() + valor_dist_norm*r.std(ddof=0))


def VaR_mejorado(r, level=5, modified= False):
    """
    Rendimientos parametricos del VaR Gaussiano en una serie o un dataframe
    si se modifica es verdadero despues se modifica el VaR de los retornos
    """
    #calcular el valor z asumiendo la distribucion normal
    z = norm.ppf(level/100)
    if modified:
        #modifica el valor z basado en la curtosis y la asimetria observada
        s = skewness(r)
        k = kurtosis(r)
        z = (z+ (z**2 -1)*s/6 + (z**3 - 3*z)*(k-3)/24-(2*z**3-5*z)*(s**2)/36)
        return -(r.mean()+z*r.std(ddof=0))
    
def semideviation3(r):
    """
    Returns the semideviation aka negative semideviation of r
    r must be a Series or a DataFrame, else raises a TypeError
    """
    excess= r-r.mean()                                        # We demean the returns
    excess_negative = excess[excess<0]                        # We take only the returns below the mean
    excess_negative_square = excess_negative**2               # We square the demeaned returns below the mean
    n_negative = (excess<0).sum()                             # number of returns under the mean
    return (excess_negative_square.sum()/n_negative)**0.5     

def C_VaR_historico(r, level =5):
    """
    Calcula el VaR condicional de las series del dataframe
    """
    if isinstance(r, pd.Series):
        is_beyond = r <= -VaR_historico(r, level=level)
        return -r[is_beyond].mean()
    elif isinstance(r, pd.DataFrame):
        return r.aggregate(C_VaR_historico, level=level)
    else:
        raise TypeError("Expected r to be a Series or DataFrame")
        
        
###############################################################################

def rendimientos_industria():
    """
    Carga los datos a utilizar en el modulo dos del rendimiento de la industria
    """
    datos = pd.read_csv("C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\data\\ind30_m_vw_rets.csv",
                        header = 0, index_col = 0)/100
    
    datos.index = pd.to_datetime(datos.index, format="%Y%m").to_period('M')
    datos.columns = datos.columns.str.strip()
    return datos

def volatilidad_anualizada(r, periodos_año):
    """
    Anualiza la volatilidad sobre los rendimientos
    se hace inferencia en los periodos por año
    """
    return r.std()*(periodos_año**0.5)

def rendimientos_anualizados(r, periodos_año):
    """
    Anualiza los rendimeintos donde se hace inferencia por año
    """
    crecimiento_compuesto  = (1+r).prod()
    numero_periodos = r.shape[0]
    return crecimiento_compuesto**(periodos_año/numero_periodos)-1

def sharpe_ratio(r, risk_free_rate, periodos_año):
    """
    Se calcula el sharpe ratio de los rendimientos anualizados
    """
    risk_free_periodo = (1+risk_free_rate)**(1/periodos_año)-1 
    excess_ret = r - risk_free_periodo
    ann_ex_ret = rendimientos_anualizados(excess_ret, periodos_año)
    ann_vol = volatilidad_anualizada(r, periodos_año)
    return ann_ex_ret/ann_vol


def var_gaussian(r, level=5, modified=False):
    """
    Returns the Parametric Gauusian VaR of a Series or DataFrame
    If "modified" is True, then the modified VaR is returned,
    using the Cornish-Fisher modification
    """
    # compute the Z score assuming it was Gaussian
    z = norm.ppf(level/100)
    if modified:
        # modify the Z score based on observed skewness and kurtosis
        s = skewness(r)
        k = kurtosis(r)
        z = (z +
                (z**2 - 1)*s/6 +
                (z**3 -3*z)*(k-3)/24 -
                (2*z**3 - 5*z)*(s**2)/36
            )
    return -(r.mean() + z*r.std(ddof=0))
    
###############################################################################
"Funciones practica 6"

def portafolio_rendimientos (weights, rendimientos):
    """
    Pesos / retornos
    indica la multiplicacion de matriz
    weights.T es la matriz transpuesta del vector de pesos dimension 1 x n
    rendimientos es un vector de rendimientos esperados dimension n x 1
    multiplicacion matricial entre ambas, donde su resultado es un unico numero escalar
    """
    return weights.T@rendimientos 


def portafolio_volatilidad(weights, covarianza):
    """
    Pesos / volatilidad
    indica la multiplicacion de matriz
    weights.T es la matriz transpuesta del vector de pesos dimension 1 x n
    rendimientos es un vector de rendimientos esperados dimension n x 1
    multiplicacion matricial entre ambas, donde su resultado es un unico numero escalar
    """
    return (weights.T@covarianza@weights)**0.5
    

    
def grafica(n_points, er, cov):
    """
    Grafica frontera eficiente de dos activos
    """
    if er.shape[0] != 2 or er.shape[0] != 2: #limita la forma de la matriz a cualquiera diferente de 2
        raise ValueError("Solo puede graficar frontera eficiente de 2 activos")
    weights = [np.array([w, 1-w]) for w in np.linspace(0, 1, n_points)]
    retornos = [portafolio_rendimientos(w, er) for w in weights]
    volatilidades = [portafolio_volatilidad(w, cov) for w in weights]
    ef = pd.DataFrame({
        "Rendimientos": retornos, 
        "Volatilidad": volatilidades
    })
    return ef.plot.line(x="Volatilidad", y="Rendimientos", style=".-")
 
    """   
def grafica_ef (n_points, er, cov, style=".-"):
    """
    #Grafica los N-activos de la frontera eficiente
    """
    weights = minimize_vol(target_return)
    retornos = [portafolio_rendimientos(w, er) for w in weights]
    volatilidades = [portafolio_volatilidad(w, cov) for w in weights]
    """   


#se recomienda hacer un constructor en este caso al tener varias funciones que 
#son dependientes la una del otro

#def objetivo(w, er):
 #   return rendimiento_objetivo - portafolio_rendimientos(w, er)
    

def volatilidad_minima(target_return, er, cov):
    """
    Rendimiento objetivo --> W 
    Se busaca hacer una funcion que optimice la funcion para poder sacar
    el mejor rendimiento al menor riesgo
    """    
    #se debe poner un objetivo
    #se deben poner restricciones
    #se debe poner una posicion incial
    n = er.shape[0]
    init_guess = np.repeat(1/n, n)
    #limites - bounds
    bounds = ((0.0, 1.0),) * n  #se crea una tupla con tuplas (tuplas no se pueden modificar son como listas)
    #restricciones a cumplir:
    """
    se quiere minimizar la volatilidad con la restriccion de que las rentabilidades 
    sean el punto optimo que se tenga en la grafica si se voltea que sea el pico maximo 
    de la curva
    """
    #Restricciones:
        #1era cada peso debe tener un equilibrio 
        #la suma de todos los pesos debe dar 1
        #no debe haber pesos negativos / son ir en corto
        #el rendimiento que se genera a partir del conjunto de pesos es el rendimiento objetivo
    weights_sum_to_1 = {'type': 'eq',
                        'fun': lambda weights: np.sum(weights) - 1}
    #se hace una funcion que indique si se comple con la restriccion o no
    return_is_target = {'type': 'eq',
                        'args': (er,),
                        'fun': lambda weights, er: target_return - portafolio_rendimientos(weights,er)} 
    #la funcion requiere de los pesos y las matrices de covarianzas
    weights = minimize(portafolio_volatilidad, init_guess,
                       args=(cov,), method='SLSQP',
                       options={'disp': False},
                       constraints=(weights_sum_to_1,return_is_target),
                       bounds=bounds)
                        #SLSQ es el optimziador
    #cualquier ponderacion que pase al optimizar, debe ser una restriccion de igualdad
    #cumple la funcion con la restriccion si el valor devuelto es cero
    
    #si se cumple el objetivo, si las ponderaciones del portafolio tienen una rentabilidad objetivo
    #igual a la rentabilidad objetivo
    return weights.x

def pesos_optimos (n_points, er, cov):
    """
    lista de los pesos para optimizar la frontera a la 
    minima volatilidad y maximo rendimiento
    """
    target = np.linspace(er.min(), er.max(), n_points)
    weights = [volatilidad_minima(target_return, er, cov) for target_return in target ]
    return weights


def grafica_frontera_eficiente(n_points, er, cov):
    """
    Grafica de la frontera eficiente con varios activos
    """
    weights = pesos_optimos(n_points, er, cov)
    rendimientos_frontera = [portafolio_rendimientos(w, er) for w in weights]
    volatilidad_frontera = [portafolio_volatilidad(w, cov) for w in weights]
    ef = pd.DataFrame({
        "Returns": rendimientos_frontera,
        "Volatility": volatilidad_frontera
    })
    return ef.plot.line(x="Volatility", y="Returns", style='.-', legend=False)

  
def sharpe_ratio_maximo(riskfree_rate, er, cov):
    """
    Se busca encontrar los pesos optimos de los activos que maximizan el sharpe ratio
    dado una tasa libre de riesgo , con los rendimientos esperados en la matriz de covarianza
    En este caso se incluye dentro de la frontera eficiente la tasa libre de riesgo 
    """
    n = er.shape[0]
    init_guess = np.repeat(1/n, n)
    bounds = ((0.0, 1.0),) * n # an N-tuple of 2-tuples!
    # construct the constraints
    weights_sum_to_1 = {'type': 'eq',
                        'fun': lambda weights: np.sum(weights) - 1
    }
    def Sharpe_ratio_negativo(weights, riskfree_rate, er, cov):
        """
        Rendimientos engativos del sharpe retion , dados los pesos
        """
        r = portafolio_rendimientos(weights, er) #rendimiento
        vol = portafolio_volatilidad(weights, cov) #volatilidad
        return -(r - riskfree_rate)/vol #se calcula el sharpe ratio negativo del portafolio
    
    #se busca minimizar el negativo del sharpe ratio 
    weights = minimize(Sharpe_ratio_negativo, init_guess,
                       args=(riskfree_rate, er, cov), method='SLSQP',
                       options={'disp': False},
                       constraints=(weights_sum_to_1,),
                       bounds=bounds)
    return weights.x

def gmv(cov):
    """
    retornos de las ponderaciones globales de la cartera de volumenes minimos del portafolio
    dado una matriz de covarianza
    """
    n = cov.shape[0]
    return sharpe_ratio_maximo(0, np.repeat(1, n), cov)


    
def grafica_frontera(n_points, er, cov, show_cml=False, style=".-"
                                   , riskfree_rate=0, show_ew=False, show_gmv = False):
    """
    Grafica la frontera eficiente con múltiples activos.
    
    Parámetros:
    - n_points: número de portafolios a simular en la frontera eficiente
    - er: rendimientos esperados (vector)
    - cov: matriz de covarianza de los activos
    - show_cml: si True, agrega la línea del mercado de capitales (CML)
    - riskfree_rate: tasa libre de riesgo (para la CML)
    - show_ew: si True, muestra el portafolio con pesos iguales (Equal Weight)
    - show_gmv: 
    """

    # Generar pesos óptimos para cada portafolio en la frontera eficiente
    weights = pesos_optimos(n_points, er, cov)
    rets = [portafolio_rendimientos(w, er) for w in weights]
    vols = [portafolio_volatilidad(w, cov) for w in weights]
    ef = pd.DataFrame({
        "Returns": rets,
        "Volatility": vols})
    
    ax = ef.plot.line(x="Volatility", y="Returns", style=style)

    # Mostrar portafolio con pesos iguales si se solicita
    if show_ew:
        n = er.shape[0]
        w_ew = np.repeat(1/n, n)
        r_ew = portafolio_rendimientos(w_ew, er)
        vol_ew = portafolio_volatilidad(w_ew, cov)
        #grafica
        ax.plot([vol_ew], [r_ew], color="red", marker="o", markersize=10)
        
    if show_gmv:
        w_gmv = gmv(cov)
        r_gmv = portafolio_rendimientos(w_gmv, er)
        vol_gmv = portafolio_volatilidad(w_gmv, cov)
        #grafica
        ax.plot([vol_gmv], [r_gmv], color="black", marker="o", markersize=10)

    # Mostrar la línea de mercado de capitales (CML) si se solicita
    if show_cml:
        ax.set_xlim(left=0)
        w_msr = sharpe_ratio_maximo(riskfree_rate, er, cov)
        r_msr = portafolio_rendimientos(w_msr, er)
        vol_msr = portafolio_volatilidad(w_msr, cov)
        cml_x = [0, vol_msr]
        cml_y = [riskfree_rate, r_msr]
        ax.plot(cml_x, cml_y, color='green', marker='o',
                linestyle='dashed', linewidth=2, markersize=12)

###############################################################################
###############################################################################

"Funciones modulo 3"

def get_ind_size():
    """
    Carga los datos de la capitalizacion bursatil de las acciones de la industria
    """
    datos = pd.read_csv("C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\data\\ind30_m_size.csv",
                        header = 0, index_col = 0)
    
    datos.index = pd.to_datetime(datos.index, format="%Y%m").to_period('M')
    datos.columns = datos.columns.str.strip()
    return datos


def get_ind_nfirms():
    """
    Carga los datos de la capitalizacion bursatil de las acciones de la industria
    """
    datos = pd.read_csv("C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\data\\ind30_m_nfirms.csv",
                        header = 0, index_col = 0)
    
    datos.index = pd.to_datetime(datos.index, format="%Y%m").to_period('M')
    datos.columns = datos.columns.str.strip()
    return datos


def run_cppi(risky_r, safe_r=None, m=3, start=1000, floor=0.8, riskfree_rate=0.03, drawdown=None):
    """
    Run a backtest of the CPPI strategy, given a set of returns for the risky asset
    Returns a dictionary containing: Asset Value History, Risk Budget History, Risky Weight History
    """
    # set up the CPPI parameters
    dates = risky_r.index
    n_steps = len(dates)
    account_value = start
    floor_value = start*floor
    peak = account_value
    if isinstance(risky_r, pd.Series): 
        risky_r = pd.DataFrame(risky_r, columns=["R"])

    if safe_r is None:
        safe_r = pd.DataFrame().reindex_like(risky_r)
        safe_r.values[:] = riskfree_rate/12 # fast way to set all values to a number
    # set up some DataFrames for saving intermediate values
    account_history = pd.DataFrame().reindex_like(risky_r)
    risky_w_history = pd.DataFrame().reindex_like(risky_r)
    cushion_history = pd.DataFrame().reindex_like(risky_r)
    floorval_history = pd.DataFrame().reindex_like(risky_r)
    peak_history = pd.DataFrame().reindex_like(risky_r)

    for step in range(n_steps):
        if drawdown is not None:
            peak = np.maximum(peak, account_value)
            floor_value = peak*(1-drawdown)
        cushion = (account_value - floor_value)/account_value
        risky_w = m*cushion
        risky_w = np.minimum(risky_w, 1)
        risky_w = np.maximum(risky_w, 0)
        safe_w = 1-risky_w
        risky_alloc = account_value*risky_w
        safe_alloc = account_value*safe_w
        # recompute the new account value at the end of this step
        account_value = risky_alloc*(1+risky_r.iloc[step]) + safe_alloc*(1+safe_r.iloc[step])
        # save the histories for analysis and plotting
        cushion_history.iloc[step] = cushion
        risky_w_history.iloc[step] = risky_w
        account_history.iloc[step] = account_value
        floorval_history.iloc[step] = floor_value
        peak_history.iloc[step] = peak
    risky_wealth = start*(1+risky_r).cumprod()
    backtest_result = {
        "Wealth": account_history,
        "Risky Wealth": risky_wealth, 
        "Risk Budget": cushion_history,
        "Risky Allocation": risky_w_history,
        "m": m,
        "start": start,
        "floor": floor,
        "risky_r":risky_r,
        "safe_r": safe_r,
        "drawdown": drawdown,
        "peak": peak_history,
        "floor": floorval_history
    }
    return backtest_result


def compound(r):
    return (1+r).prod()-1
    

def compound2(r):
    return np.expm1(np.log1p(r).sum())


def summary_stats(r, risk_free_rate=0.03):
    """
    Regresa un dataframe que contenga la sumatoria agregada de los rendimientos
    de las columnas de r 
    """
    ann_r = r.aggregate(rendimientos_anualizados, periodos_año = 12)
    ann_vol = r.aggregate(volatilidad_anualizada, periodos_año = 12)
    ann_sr = r.aggregate(sharpe_ratio, risk_free_rate = risk_free_rate, periodos_año =12)
    dd = r.aggregate(lambda r: drawdown(r).Drawdown.min())
    skew = r.aggregate(skewness)
    kurt = r.aggregate(kurtosis)
    cf_var5 = r.aggregate(VaR_mejorado, modified=True)
    hist_cvar5 = r.aggregate(C_VaR_historico)
    return pd.DataFrame({
        "Annualized Return": ann_r,
        "Annualized Vol": ann_vol,
        "Skewness": skew,
        "Kurtosis": kurt,
        "Cornish-Fisher VaR (5%)": cf_var5,
        "Historic CVaR (5%)": hist_cvar5,
        "Sharpe Ratio": ann_sr,
        "Max Drawdown": dd
    })
    
    
    
def gbm(n_years = 10, n_scenarios=1000, mu=0.07, sigma=0.15, steps_per_year=12, s_0=100.0, prices=True):
    """
    Evolution of Geometric Brownian Motion trajectories, such as for Stock Prices through Monte Carlo
    :param n_years:  The number of years to generate data for
    :param n_paths: The number of scenarios/trajectories
    :param mu: Annualized Drift, e.g. Market Return
    :param sigma: Annualized Volatility
    :param steps_per_year: granularity of the simulation
    :param s_0: initial value
    :return: a numpy array of n_paths columns and n_years*steps_per_year rows
    """
    # Derive per-step Model Parameters from User Specifications
    dt = 1/steps_per_year
    n_steps = int(n_years*steps_per_year) + 1
    # the standard way ...
    # rets_plus_1 = np.random.normal(loc=mu*dt+1, scale=sigma*np.sqrt(dt), size=(n_steps, n_scenarios))
    # without discretization error ...
    rets_plus_1 = np.random.normal(loc=(1+mu)**dt, scale=(sigma*np.sqrt(dt)), size=(n_steps, n_scenarios))
    rets_plus_1[0] = 1
    ret_val = s_0*pd.DataFrame(rets_plus_1).cumprod() if prices else rets_plus_1-1
    return ret_val


#calcula una tasa de descuento al tiempo t y una tasa anual
def discount_a(t, r):
    """
    Calcula el precio de un bono a puro desacuento que paga en dolares a tiempo 
    t donde t son los años y r es la tasa de interes anual
    """
    return (1+r)**(-t)


#calcula una tasa de descuento al tiempo t y una tasa anual iterando los valores
#del interes r y del tiempo t
def discount(t, r):
    """
    Compute the price of a pure discount bond that pays a dollar at time period t
    and r is the per-period interest rate
    returns a |t| x |r| Series or DataFrame
    r can be a float, Series or DataFrame
    returns a DataFrame indexed by t
    """
    discounts = pd.Series([(r+1)**-i for i in t]) #genera un bucle con i para iterar los valores de i
    discounts.index = t
    return discounts


def pv(flows, r):
    """
    Calcula el valor presente de la secuncia de los flujos de efectivo dados por un indice y
    unos montos, puede ser un escalar o una serie de un data frame con un numero de columnas 
    por los flujos
    """
    dates = flows.index
    discounts = discount(dates, r)
    return discounts.multiply(flows, axis='rows').sum()
    
def funding_ratio(assets, liabilities, r):
    """
    Computes the funding ratio of a series of liabilities, based on an interest rate and current value of assets
    """
    return pv(assets, r)/pv(liabilities, r)

def tasa_anual(r):
    """Convierte una tasa instantánea a una tasa anual compuesta continuamente."""
    return np.expm1(r)

def tasa_instantanea(r):
    """Convierte una tasa anual compuesta a una tasa instantánea."""
    return np.log1p(r)



def Cox_Ingerson_Ross_Model(n_scenarios, n_years=10, a=0.05, b=0.03, sigma=0.05, steps_per_year=12, r_0=None):
    """
    Implementaciones para el modelo CIR para las tasas de interes 
    """
    if r_0 is None: r_0 = b
    r_0 = tasa_instantanea(r_0)
    dt = 1/steps_per_year
    num_steps = int(n_years*steps_per_year)+1
    shock = np.random.normal(0, scale = np.sqrt(dt), size=(num_steps, n_scenarios))
    rates= np.empty_like(shock)
    rates[0] = r_0
    for step in range(1, num_steps):
        r_t = rates[step-1]
        d_r_t = a *(b-r_t)*dt + sigma*np.sqrt(r_t)*shock[step]
        rates[step]= abs(r_t + d_r_t)
        
    return pd.DataFrame(data = tasa_anual(rates), index=range(num_steps))
    

def cir(n_years = 10, n_scenarios=1, a=0.05, b=0.03, sigma=0.05, steps_per_year=12, r_0=None):
    """
    Generate random interest rate evolution over time using the CIR model
    b and r_0 are assumed to be the annualized rates, not the short rate
    and the returned values are the annualized rates as well
    """
    if r_0 is None: r_0 = b 
    r_0 = tasa_instantanea(r_0)
    dt = 1/steps_per_year
    num_steps = int(n_years*steps_per_year) + 1 # because n_years might be a float
    
    shock = np.random.normal(0, scale=np.sqrt(dt), size=(num_steps, n_scenarios))
    rates = np.empty_like(shock)
    rates[0] = r_0

    h = math.sqrt(a**2 + 2*sigma**2)
    prices = np.empty_like(shock)

    def price(ttm, r):
        _A = ((2*h*math.exp((h+a)*ttm/2))/(2*h+(h+a)*(math.exp(h*ttm)-1)))**(2*a*b/sigma**2)
        _B = (2*(math.exp(h*ttm)-1))/(2*h + (h+a)*(math.exp(h*ttm)-1))
        _P = _A*np.exp(-_B*r)
        return _P
    prices[0] = price(n_years, r_0)
    
    for step in range(1, num_steps):
        r_t = rates[step-1]
        d_r_t = a*(b-r_t)*dt + sigma*np.sqrt(r_t)*shock[step]
        rates[step] = abs(r_t + d_r_t)
        # generate prices at time t as well ...
        prices[step] = price(n_years-step*dt, rates[step])

    rates = pd.DataFrame(data=tasa_anual(rates), index=range(num_steps))
    prices = pd.DataFrame(data=prices, index=range(num_steps))
    return rates, prices



def Cox_Ingersoll_Ross_Model_Precios(r_0=0.03, a=0.5, b=0.03, sigma=0.001, n_scenarios=2, T=10, n_steps=100):
    dt = T / n_steps
    rates = np.zeros((n_steps + 1, n_scenarios))
    rates[0] = r_0

    for t in range(1, n_steps + 1):
        z = np.random.normal(size=n_scenarios)
        rates[t] = (
            rates[t - 1] +
            a * (b - rates[t - 1]) * dt +
            sigma * np.sqrt(np.maximum(rates[t - 1], 0)) * np.sqrt(dt) * z
        )

###############################################################################
###############################################################################
###############################################################################

"Parametros del bono"

#maturity : es el tiempo de vencimiento del bono, cuanto va durar hasta que vence
#principal : es el valor del instrumento llamado tambien valor nominal o face value
#coupon_rate : es la tasa cupon anual, si es de 5% al año pagara 5% de tasa cupon, la tasa cupon se paga a lo largo del año
#coupons_per_year : indica cuantos cupones hay por año, normalmente son 2 semestreales de (5% /2) = 2.5% semestral
#la tasa de descunto: se usa para descontar todos los cupones 


#el cupon si se apga 12 veces es uno semestra y se divide la tasa anual entre 12 
#por lo que si dura 3 años de maturity pagaria alrededor de 36 cupones el bono
def bond_cash_flows(maturity, principal=100, coupon_rate=0.03, coupons_per_year=12):
    """
    Rendimientos de las series de los cash flows generados por bono
    indexado  a un determinado numero de cupones
    """
    n_coupons = round(maturity*coupons_per_year)
    coupon_amt = principal*coupon_rate/coupons_per_year
    coupons = np.repeat(coupon_amt, n_coupons)
    coupon_times = np.arange(1, n_coupons+1)
    cash_flows = pd.Series(data=coupon_amt, index=coupon_times)
    cash_flows.iloc[-1] += principal
    return cash_flows


#funcion para valuar el bono / genera los flujos de efectivo simplemente 
#desopues calcula el valor presente
def bond_price (maturity, principal=100, coupon_rate=0.03, coupons_per_year=12, discount_rate = 0.03):
    """
    Precio de un bono basado en los parametros de vencimineot del bono, 
    principal es el valor nominal , teniendo tambien como parametros 
    la tasa cupon, los cupones por añop, y la tasa de inter5es descontada de los
    flujos de efectivo futuros asociada al valor presente, cuanto valen hoy los 
    que se recibiran en el futuro
    """
    cash_flows = bond_cash_flows(maturity, principal, coupon_rate, coupons_per_year)
    return pv(cash_flows, discount_rate/coupons_per_year)



def macaulay_duration(flows, discount_rate):
    """
    Calcula la duracion de Macaulay en secuencia de los flujos de efecetivo
    o cash flows.
    """
    flujo_descontado = discount(flows.index, discount_rate)*flows
    weights = flujo_descontado/flujo_descontado.sum()
    return np.average(flows.index, weights=weights)
    

def match_durations(cf_t, cf_s, cf_l, discount_rate ):
    """
    Retornos de los pesos de W en cf_s a lo largo de (1-w), en cf_l tencdra una 
    duracion efectiva que haga conexion con cf-t
    """
    d_t = macaulay_duration(cf_t, discount_rate)
    d_s = macaulay_duration(cf_s, discount_rate)
    d_l = macaulay_duration(cf_l, discount_rate)
    return (d_l - d_t)/(d_l - d_s)
    
    
def bond_total_return(monthly_prices, principal, coupon_rate, coupons_per_year):
    """
    Computes the total return of a Bond based on monthly bond prices and coupon payments
    Assumes that dividends (coupons) are paid out at the end of the period (e.g. end of 3 months for quarterly div)
    and that dividends are reinvested in the bond
    """
    coupons = pd.DataFrame(data = 0, index=monthly_prices.index, columns=monthly_prices.columns)
    t_max = monthly_prices.index.max()
    pay_date = np.linspace(12/coupons_per_year, t_max, int(coupons_per_year*t_max/12), dtype=int)
    coupons.iloc[pay_date] = principal*coupon_rate/coupons_per_year
    total_returns = (monthly_prices + coupons)/monthly_prices.shift()-1
    return total_returns.dropna()



def terminal_stats(rets, floor = 0.8, cap=np.inf, name="Stats"):
    """
    Genera un resumen de las estadisticas en la terminal de valores por 
    dolar invertiudo, atraves de un rango de N escenarios
    rets es el dataframe de rendimientos T X N donde T es el tiempo por paso (asumiendo quye los rets son ordenados por tiempo)
    los retornos son una columna de un dataframe del resumen de las estadisticas indexados por 
    """
    terminal_wealth = (rets+1).prod()
    breach = terminal_wealth < floor
    reach = terminal_wealth >= cap
    p_breach = breach.mean() if breach.sum() > 0 else np.nan
    p_reach = reach.mean() if reach.sum() > 0 else np.nan
    e_short = (floor-terminal_wealth[breach]).mean() if breach.sum() > 0 else np.nan
    e_surplus = (-cap+terminal_wealth[reach]).mean() if reach.sum() > 0 else np.nan
    sum_stats = pd.DataFrame.from_dict({
        "mean": terminal_wealth.mean(),
        "std" : terminal_wealth.std(),
        "p_breach": p_breach,
        "e_short":e_short,
        "p_reach": p_reach,
        "e_surplus": e_surplus
    }, orient="index", columns=[name])
    return sum_stats


