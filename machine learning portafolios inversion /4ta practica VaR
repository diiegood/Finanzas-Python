"""
  Primera parte modulo principal donde se van a cargar los modulos del VaR
"""


import pandas as pd 
import funciones_curso as erk
import scipy.stats
import numpy as np
from scipy.stats import norm 


#hfi son los datos que se cargan a traves de una libreria creada en el modulo anterior

"Se cargan las librerias en esta funcion"

hfi = erk.returns() #carga la funcion para leer los datos como hfi
hfi.head()

###############################################################################

pd.concat([hfi.mean(), hfi.median(), hfi.mean()>hfi.median()], axis="columns")

#valor 0 corresponde a la media
#valor 1 corresponde a la mediana
#valor 2 corresponde a ver si la media es menor a la mediana en boleano

#para calcular la curtosis de la base de datos

erk.skewness(hfi).sort_values() #va a ordenar los valores de la funcion skewwnes aplicado a hfi


scipy.stats.skew(hfi)
erk.skewness(hfi)
hfi.shape

normal_rets = np.random.normal(0, .15, size = (26300, 1))
normal_rets

erk.skewness(normal_rets) #calcula la skewness de los datos anteriormente creados aleatoriamente

erk.kurtosis(normal_rets)  #calcula la curtosis de los datos anteriormente creados aleatoriamente

erk.skewness(hfi) #calcula la asimetria de los datos

erk.kurtosis(hfi)

scipy.stats.kurtosis(normal_rets) #calcula la curtosis 

scipy.stats.jarque_bera(normal_rets) #calcula si es normal o no con el jarque bera
#primer numero indica valor  jarque bera  y el segundo es el p-value del test de hipotesis (valor de confianza)
#rechaza o acepta la hipotesis nula de la distribucion normal


scipy.stats.jarque_bera(hfi) 

#para ver si la distribucion es normal
erk.is_normal(normal_rets) #marca un boleano si es normal o no la distribucion de datos
erk.is_normal(hfi)
hfi.aggregate(erk.is_normal)

###############################################################################

"Practica 4"

"Practica 3"

import pandas as pd 
import funciones_curso as erk
import scipy.stats 
from scipy.stats import norm
import numpy as np


#hfi son los datos que se cargan a traves de una libreria creada en el modulo anterior

"Se cargan las librerias en esta funcion"

hfi = erk.get_hfi_returns() #carga la funcion para leer los datos como hfi
hfi.head()

#indica la dispersion de la media o la desviacion estandar
hfi.std(ddof= 0)

#se calcula la desviacion estandar para el subconjunto hfi
hfi[hfi<0].std(ddof=0) #calcula la semi-desviacion

erk.semi_desviacion(hfi)

"Calculo del VaR y C-VaR"

#VaR historico  ###############################################################

"Requiere un historial de los rendimeintos intentando encontrar el VaR del 5% (dado un CI de 95%)"
#observa las rentabilidades pasadas y las analiza desde el percentil 5, 
#siendo el limite dado el intervalo de confianza seleccionado

np.percentile(hfi, 5) #se observa el valor en percentil 5 en columna
np.percentile(hfi, 5, axis= 0) #muestra los percentiles de todas las columnas
#Hay una probabilidad de 5% de que en el periodo determinado se pierda el primer valor obtenido del array anterior

erk.VaR_historico(hfi)  #Calcula el VaR para cada columna dado el percentil 5
#q- percentil 

norm.ppf(0.05) #esta funcion nos da el valor asociado de la distribucion normal 
#segun el valor de la distribucion que esta debajo del numero que pongamos

"""
si pongo 0.5 es el valor de la media de la normal porque significa que esta en el (0.5) 50%
por lo que deberia ser 0 ya que estandarizada su media es cero
"""

distrbucion_normal_puntuacion = norm.ppf(0.05)
distrbucion_normal_puntuacion

#Calcula el VaR a un nivel del 5% osea en el percentil 5
-(hfi.mean() + distrbucion_normal_puntuacion * hfi.std(ddof=0))

#si se quiere poner positivo el resultado se agrega  -()  al exterior de la funcion

#VaR parametrico-Gaussiano / se toma como una muestra #########################

"Supone que la distribucion es normal y parametrico es segun los parametros de quien realiza el calculo"

erk.VaR_Normal(hfi) #Calcula el VaR de los datos usando modelo parametrico gaussiano

#calcula el valor z suponiendo que los datos siguen la distribucion normal, pero en realidad no 
#porque tienen asimetria y exceso de curtosis

#Lo ideal seria ajustar los datos para tener en cuenta la asimetria y la curtosis

#La normal la curtosis siempre es 3 si es mayor de 3 no es normal

#VaR con  Modificacion Cornish-Fisher / #######################################

"""se toma como que los retornos tienen sesgo negativo o positivo, 
#teniendo colas mas pesadas de lo que la distribucion normal refleja"""
#VaR

#se genera una lista de resultados donde vienen todos los modelos del VaR 
VaR_list = [erk.VaR_Normal(hfi),erk.VaR_mejorado(hfi, modified = True), erk.VaR_historico(hfi)]

#para comparar todos los modelos de var se genera una matriz con la suma de todas las matrices
#de los resultados anteriores de los modelos
comparasion = pd.concat(VaR_list, axis=1)

#se renombran las columnas de la matriz comparasion creada posteriormente
comparasion.columns=["dist_normal", "Cornish-Fisher", "Historico"]

comparasion.plot.bar (title= "Hedge Fund Indices: VaR")

erk.C_VaR_historico(hfi)
#Estos numeros dicen que si ocurre ese cinco porciento de probabilidades, siendo
#el peor 5% de los casos posibles,  el promedio de la perdida 
#es el de cada valor arrojado por cada variable.

####################################################################################
####################################################################################
####################################################################################
####################################################################################

"""
Segunda parte nombre del modulo ## funciones_curso ## cuerpo de donde se generan las funciones  
"""

"""
Nota:
    
Al crear una funcion se debe cargar el modulo donde se encuentra y a su vez cargar la funcion a usar
cada vez que se crea una se debe reiniciar el programa para que la pueda correr sin problemas
correr primero el modulo de funciones y despues el principal.
"""

import pandas as pd
import numpy as np
from scipy.stats import norm

def drawdown(return_series: pd.Series):
    """Takes a time series of asset returns.
       returns a DataFrame with columns for
       the wealth index, 
       the previous peaks, and 
       the percentage drawdown
    """
    wealth_index = 1000*(1+return_series).cumprod()
    previous_peaks = wealth_index.cummax()
    drawdowns = (wealth_index - previous_peaks)/previous_peaks
    return pd.DataFrame({"Wealth": wealth_index, 
                         "Previous Peak": previous_peaks, 
                         "Drawdown": drawdowns})


def obtener_returnos():
    """
    Load the Fama-French Dataset for the returns of the Top and Bottom Deciles by MarketCap
    1- carga los datos panel
    2-filtra solo los valores de Lo-20 y Hi-20 del data frame
    3-se calculan los rendimientos de rendimientos/100
    4-se cambia el formato a serie de tiempo y se indexa el periodo
    5-regresa la funcion rets que es el dataframe ya filtrado con todo lo anterior
    """
    #se cargan los datos panel
    me_m = pd.read_csv(
       "C:/Users/creep/OneDrive/Escritorio/programacion/data/Portfolios_Formed_on_ME_monthly_EW.csv",
       header=0,
       index_col=0,
       parse_dates=True,
       na_values=-99.99)
    rets = me_m[['Lo 20', 'Hi 20']]
    #rets.columns = ['SmallCap', 'LargeCap']
    rets = me_m
    rets = rets/100
    rets.index = pd.to_datetime(rets.index, format="%Y%m").to_period('M')
    return rets


def returns():  #funcion para leer base de datos
    """
    Load and format the EDHEC Hedge Fund Index Returns
    """
    hfi = pd.read_csv("C:/Users/creep/OneDrive/Escritorio/programacion/data/edhec-hedgefundindices.csv",
                      header=0, index_col=0, parse_dates=True)
    hfi = hfi/100
    hfi.index = hfi.index.to_period('M')
    return hfi


def skewness(r):
    """
    Alternative to scipy.stats.skew()
    Computes the skewness of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean()
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**3).mean()
    return exp/sigma_r**3


def kurtosis(r):
    """
    Alternative to scipy.stats.kurtosis()
    Computes the kurtosis of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean() 
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**4).mean()
    return exp/sigma_r**4


import scipy.stats
def is_normal(r, level=0.01):
    """
    Applies the Jarque-Bera test to determine if a Series is normal or not
    Test is applied at the 1% level by default
    Returns True if the hypothesis of normality is accepted, False otherwise
    """
    if isinstance(r, pd.DataFrame):
        return r.aggregate(is_normal)
    else:
        statistic, p_value = scipy.stats.jarque_bera(r)
        return p_value > level

def semi_desviacion(r):
    """
    los rendimientos con semi-desviacion o semidesvaicion negativa de r
    deben ser series o un data frame
    """
    is_negative = r<0
    return r[is_negative].std(ddof=0)

#r son los rendimientos
#level es el valor del percentil donde se va parar para sacar el VaR
def VaR_historico(r, level=5):
    """
    VaR historico
    """
    if isinstance(r, pd.DataFrame):
        return r.aggregate(VaR_historico, level=level)
    
    elif isinstance(r, pd.Series):
        return -np.percentile(r, level)
    else:
        raise TypeError("no seas wey")

      
    
def VaR_Normal (r, level=5):
    """
    Calcula el VaR con retornos usando modelo parametrico de la distribucion normal
    """
    #para calcular el valor z de la distribucion si fuera normal
    valor_dist_norm = norm.ppf(level/100)
    return -(r.mean() + valor_dist_norm*r.std(ddof=0))


def VaR_mejorado(r, level=5, modified= False):
    """
    Rendimientos parametricos del VaR Gaussiano en una serie o un dataframe
    si se modifica es verdadero despues se modifica el VaR de los retornos
    """
    #calcular el valor z asumiendo la distribucion normal
    z = norm.ppf(level/100)
    if modified:
        #modifica el valor z basado en la curtosis y la asimetria observada
        s = skewness(r)
        k = kurtosis(r)
        z = (z+ (z**2 -1)*s/6 + (z**3 - 3*z)*(k-3)/24-(2*z**3-5*z)*(s**2)/36)
        return -(r.mean()+z*r.std(ddof=0))
    
def semideviation3(r):
    """
    Returns the semideviation aka negative semideviation of r
    r must be a Series or a DataFrame, else raises a TypeError
    """
    excess= r-r.mean()                                        # We demean the returns
    excess_negative = excess[excess<0]                        # We take only the returns below the mean
    excess_negative_square = excess_negative**2               # We square the demeaned returns below the mean
    n_negative = (excess<0).sum()                             # number of returns under the mean
    return (excess_negative_square.sum()/n_negative)**0.5     

def C_VaR_historico(r, level =5):
    """
    Calcula el VaR condicional de las series del dataframe
    """
    if isinstance(r, pd.Series):
        is_beyond = r <= -VaR_historico(r, level=level)
        return -r[is_beyond].mean()
    elif isinstance(r, pd.DataFrame):
        return r.aggregate(C_VaR_historico, level=level)
    else:
        raise TypeError("Expected r to be a Series or DataFrame")
        
        
###############################################################################

def cargar_datos():
    """
    Carga los datos a utilizar en el modulo dos del rendimiento de la industria
    """
    datos = pd.read_csv("C:\\Users\\creep\\OneDrive\\Escritorio\\programacion\\data\\ind30_m_vw_rets.csv",
                        header = 0,
                        index_col = 0,
                        parse_dates=True)/100
    datos.index = pd.to_datetime(datos.index, format="%Y%m").to_period("M")
    datos.columns = datos.columns.str.strip()
    return datos


def volatilidad_anualizada(r, periodos_año):
    """
    Anualiza la volatilidad sobre los rendimientos
    se hace inferencia en los periodos por año
    """
    return r.std()*(periodos_año**0.5)

def rendimientos_anualizados(r, periodos_año):
    """
    Anualiza los rendimeintos donde se hace inferencia por año
    """
    crecimiento_compuesto  = (1+r).prod()
    numero_periodos = r.shape[0]
    return crecimiento_compuesto**(periodos_año/numero_periodos)-1

def sharpe_ratio(r, risk_free_rate, periodos_año):
    """
    Se calcula el sharpe ratio de los rendimientos anualizados
    """
    risk_free_periodo = (1+risk_free_rate)**(1/periodos_año)-1 
    excess_ret = r - risk_free_periodo
    ann_ex_ret = rendimientos_anualizados(excess_ret, periodos_año)
    ann_vol = volatilidad_anualizada(r, periodos_año)
    return ann_ex_ret/ann_vol


def var_gaussian(r, level=5, modified=False):
    """
    Returns the Parametric Gauusian VaR of a Series or DataFrame
    If "modified" is True, then the modified VaR is returned,
    using the Cornish-Fisher modification
    """
    # compute the Z score assuming it was Gaussian
    z = norm.ppf(level/100)
    if modified:
        # modify the Z score based on observed skewness and kurtosis
        s = skewness(r)
        k = kurtosis(r)
        z = (z +
                (z**2 - 1)*s/6 +
                (z**3 -3*z)*(k-3)/24 -
                (2*z**3 - 5*z)*(s**2)/36
            )
    return -(r.mean() + z*r.std(ddof=0))
    
    
    
    
    
    
    
    
    
    
    
