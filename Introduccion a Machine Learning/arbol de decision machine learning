"Machine learning" 
"Regresion de arbol de decision"  #MODELO NO PARAMETRICO 

#conjunto de modelos parametricos -> se asume una relacion causal o funcion preestablecidas, no importa la interpretabilidad
#si no que el modelo funcione bastante y se adapte bastante bien a los datos disminuyendo el error

#conjunto de modelos no parametricos -> el modelo perderia interpretabilidad pero no hay relacion causal
#se disminuye el error que pueda tener, pero la regresion lineal , no se va conocer dependiendo del modelo
#aveces no va tener la interpretabilidad que se espera, mejores predictores de modelos.

#lo que interesa que haya un modelo que tenga gran interpretabilidad que se peuda explicar
#que tengan sentidos los resultados, explicandose de forma sencilla, 

#!pip install pydotplus  / libreria para machine learning

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from IPython.display import Image 

from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import (
    BaggingClassifier, RandomForestClassifier,
    BaggingRegressor, RandomForestRegressor,
    GradientBoostingRegressor
)
from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report

import six
import sys
import pydotplus
from io import StringIO  # En lugar de six.StringIO
plt.style.use("ggplot")


#se crea una funcion para el arbol
def print_tree (estimator, features, class_names=None, filled=True):
    tree = estimator
    names= features
    color = filled
    classn = class_names
    
    dot_data = StringIO()
    export_graphviz(estimator, out_file = dot_data, feature_names = features, class_names = classn,
                    filled = filled)
    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
    return(graph)



df = pd.read_csv("https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/d9bcf657f97201394a59fffd801c44347eb7e28d/Hitters.csv")
df = df.dropna()
df.info()
df.head(30)
#la variable a predecir es el salario.

X = df[['Years', 'Hits']] #regresores son los años en la liga jugando y los hits que ha hecho cada jugador
Y = np.log(df.Salary)

#se grafican las variables de salario y de logaritmo de salario
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (11,4))
ax1.hist(df.Salary)
ax1.set_xlabel('Salary')
ax2.hist(Y)
ax2.set_xlabel('log(Salary)')

#los datos estan concentrados a la izquierda pero tiene cesgo 
#los datos estan muy hacia la derecha

#se hace la regresion del arbol de decision / saber cuantos nodos o la profundidad que va tener
dtree = DecisionTreeRegressor(max_leaf_nodes = 3) #profundidad del arbol de decision.
dtree.fit(X, Y)

###############################################################################
#grafica que requiere instalar la paqueteria graphviz
#se busca en internet la grafica graphviz para poder instalarlo directamente de internet para correrlo en python
dot_data = StringIO()
export_graphviz(dtree, 
                out_file=dot_data,
                filled = True,
                rounded = True, 
                special_characters= True)

graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(graph.create_png())
###############################################################################

# X0 son los años del jugador con experiencia
#X1 son los hits del jugador que hace en cada partido


#se grafica como funcionan las divisiones para tener los jugadores segun su salario
df.plot("Years", "Hits", kind= "scatter", color="orange", figsize= (7,6))
plt.xlim(0,25)
plt.ylim(ymin = -5)
plt.xticks([1, 4.5, 24])
plt.yticks([1, 117.5, 238])
plt.vlines(x=4.5, ymin=-5, ymax=250, colors='blue', linestyles='dashed')
plt.hlines(117.5, xmin =4.5, xmax = 250, colors='orange', linestyles = 'dashed')
plt.annotate('R1', xy=(2,117.5), fontsize='xx-large')
plt.annotate('R2', xy=(11,60), fontsize="xx-large")
plt.annotate("R3", xy=(11,170), fontsize="xx-large")

#Este ejemplo tiene una interpretavilidad bastante alta pero   
#se han usado pocas variables regresoras (experiencia y hits), para predecir el salario
#entre mas variables regresoras se agreguen mejor sera el modelo mas preciso aunque se pierde mucho mas interpretabilidad.









